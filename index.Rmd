---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Sara Blanco, seb4296

### Introduction 

Throughout this project I used the candy_rankings dataset found in the fivethirtyeight package. I found this data by exploring the preloaded R datasets and was immediately intrigued by the thought of classifying candy! Halloween is my favorite holday, so I was very excited to be able to apply this data to the concepts we've explored throughout this course. 

This dataset was initially used to rank Halloween candy and contains the name (competitorname), types and features of the candy (including chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, and pluribus), and the percentages (sugarpercent, pricepercent, winpercent) for each candy. The variables measure whether or not each candy contains features (chocolate, nougat, etc.) and also the percentile of sugar and price along with the overall "win percentage" as determined by a matchup study. There are 85 observations and the values of True and False for each binary variable are listed below. 


```{R}
library(tidyverse)
library(fivethirtyeight)
candy_rankings

candy_rankings %>% select(chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus) %>% sapply(table)
```

### Cluster Analysis

```{R}
library(cluster)

#Process data 
clust_dat<-candy_rankings %>% dplyr::select(sugarpercent, pricepercent, winpercent)

#PAM
pam1 <- clust_dat %>% pam(k=3) 
pam1

#Visualize
pamclust<-clust_dat %>% mutate(cluster=as.factor(pam1$clustering))
pamclust %>% ggplot(aes(sugarpercent, winpercent, color=cluster)) + geom_point()

#Summarize clusters
pamclust %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)

#Final medoids
candy_rankings%>%slice(pam1$id.med)

#Average silhouette width
pam1$silinfo$avg.width
plot(pam1,which=2)

```
    
When initially performing clustering, I used three variables (sugarpercent, pricepercent, and winpercent) and a k value of 3 to perform PAM. The datapoints were then assigned to one of the three clusters and summarized to give the means for each of the three variables. I also pulled out the medoids for each cluster to show which candies were most representative in each group. PAM clustering classified this data with 0.552 accuracy represented in average silhouette width, which can be quantified as a reasonable structure. 

```{R}
#Picking number of clusters
pam_dat<-candy_rankings%>%select(sugarpercent, pricepercent, winpercent)
sil_width<-vector()
for(i in 2:10){  
  pam_fit <- pam(pam_dat, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

#PAM
pam2 <- clust_dat %>% pam(k=2) 
pam2

#Visualize
pamclust2<-clust_dat %>% mutate(cluster=as.factor(pam2$clustering))
pamclust2 %>% ggplot(aes(sugarpercent, winpercent, color=cluster)) + geom_point()

#Summarize clusters
pamclust2 %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)

#Final medoids
candy_rankings%>%slice(pam2$id.med)

#Average silhouette width/goodness of fit
pam2$silinfo$avg.width
plot(pam2,which=2)

#ggpairs
library(GGally); ggpairs(pamclust2, columns=1:4, aes(color=cluster))

```

At this point, we used a plot to find the largest silhouette width, which turned out to be 2, and applied this to our data. As a note, I attempted to scale the data, but this severely altered the strength of the structure, so I opted to skip this step. This model increased the average silhouette width to 0.590, which is still reasonable and slightly stronger! According to ggpairs, it appears that cluster 1 has a higher sugarpercent, high pricepercent, and high winpercent. Cluster 2 appears to have a low sugarpercent, lower pricepercent, and low winpercent.  

```{r}
#Incorporate categorical variable: gower
dat1<-candy_rankings%>%mutate_if(is.logical,as.factor)%>%column_to_rownames("competitorname") %>%
  select(-fruity, -caramel, - peanutyalmondy, -nougat, -crispedricewafer, -hard, -bar, -pluribus)

gower1<-daisy(dat1,metric="gower")


#Number of clusters
sil_width<-vector()
for(i in 2:10){  
  pam_fit <- pam(gower1, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)


#PAM
pam3 <- pam(gower1, k = 2, diss = T) 
pam3

#Summary Statistics
dat1%>%mutate(cluster=pam3$clustering)%>%group_by(cluster)%>%
  rename_all(function(x)str_replace(x,"_",""))%>%
  summarize_if(is.numeric,.funs = list("mean"=mean,"median"=median,"sd"=sd),na.rm=T)%>%
  pivot_longer(contains("_"))%>%
  separate(name,sep="_",into=c("variable","stat"))%>%
  pivot_wider(names_from = "variable",values_from="value")%>%arrange(stat)

#Medoids: most representative of their cluster
candy_rankings %>% slice(pam3$id.med)

#Interpreting fit/average silhouette width
pam3$silinfo$avg.width
plot(pam3,which=2)

#Visualizing: ggpairs
library(GGally); ggpairs(dat1, columns=1:4, aes(color=as.factor(pam3$clustering)))

```
    
I chose to cluster based on gower dissimilarities as well and incorporated the categorical variable "chocolate" to see how the dataset performed. I used the silhouette width of 2 and calculated gower dissimilarities to see how the dataset would cluster; chocolate and non-chocolate clusters seemed to result. The average silhouette width ended up being 0.729 which can be deemed a strong structure. The ggpairs showed that cluster 1 fell higher on sugarpercent, high on pricepercent, and high on winpercent, whereas cluster 2 was lower on all three variables when chocolate was considered. 


### Dimensionality Reduction with PCA

```{R}
candy_p <- candy_rankings %>% select(competitorname, sugarpercent, pricepercent, winpercent)
candy <- candy_p %>% select_if(is.numeric) %>% scale
rownames(candy) <- candy_rankings$competitorname
candy_pca <- princomp(candy)
names(candy_pca)
candy_pca

#Chossing number of PCs
eigval <-  candy_pca$sdev^2
varprop=round(eigval/sum(eigval), 2) 

ggplot() + geom_bar(aes(y=varprop, x=1:3), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:3)) + 
  geom_text(aes(x=1:3, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + 
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + 
  scale_x_continuous(breaks=1:10)
summary(candy_pca, loadings=T)


candydf<-data.frame(Name= candy_rankings$competitorname, PC1=candy_pca$scores[, 1],PC2=candy_pca$scores[, 2])

ggplot(candydf, aes(PC1, PC2)) + geom_point()

library(factoextra)
fviz_pca_biplot(candy_pca, col.var="deeppink3") +
   xlim(-4, 4) + ylim (-3, 3)

```

Based on the plot of variance, I chose to keep the first two Principal Components. PC1 appears to be a general factor to assess the general goodness. All of the events within this component are positive, so a high score here means the candy is generally well liked, whereas a low score means the candy does poorer in each category. PC2 looks like it focuses on sugarpercent vs. winpercent. A higher score here means more sugar and a lower win percentage whereas a lower score may mean a higher win percentage and lower sugar percent. These two PCs account for 80% of the total variance in this dataset as seen in the plot. I chose to incorporate a biplot as a visual to show the candies included in our dataset and where they fall. You can see that names of candies like "Root Beer Barrels" and "Sour Patch Kids" which fall lower on Dimension 1 whereas the chocolate candies like "Kit Kat" and "Twix" are higher on this measure. 



###  Linear Classifier

```{R}
y_hat <- sample(c("TRUE","FALSE"), size=length(candy_rankings$chocolate), replace=T)
candy_rankings %>% select(competitorname, chocolate,sugarpercent, pricepercent, winpercent) %>% mutate(prediction=y_hat)

candy_rankings$chocolateIn <- as.integer(candy_rankings$chocolate)
fit <- glm(chocolateIn ~ sugarpercent + pricepercent + winpercent, data=candy_rankings, family="binomial")
score <- predict(fit, type="response")
#predictions for all observations
score %>% round(3) 
class_diag(score,truth=candy_rankings$chocolateIn,positive=1)
table(truth=factor(candy_rankings$chocolateIn==1, levels=c("TRUE", "FALSE")), predictions=factor(score>.5, levels=c("TRUE", "FALSE")))


candy_rankings %>% ggplot(aes(sugarpercent, chocolateIn)) + geom_point(aes(color=score > .5)) + geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

```

```{R}
# cross-validation of linear classifier 
#K-fold CV

k=10
data<-candy_rankings[sample(nrow(candy_rankings)),]
folds<-cut(seq(1:nrow(candy_rankings)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$chocolateIn 
  fit <- glm(chocolateIn ~ sugarpercent + pricepercent + winpercent, data=train, family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}
summarize_all(diags,mean)

```

We started off by predicting chocolate from the three numeric variables in our dataset to give predictions for all of the values in the dataset. Next, we trained the model using logistic regression and noted predictions for every observation. This gave an in-sample performance that was rather strong, with an AUC of 0.9105. Next, I completed k-fold CV to determine the out-of-sample performance, which appeared to also be rather good! The AUC for this model is 0.92832, meaning that the model predicts new observations fairly well. Because the model did better in cross validation, we can conclude that the AUCs are comparable and there are no signs of overfitting. 



### Non-Parametric Classifier

```{R}
library(caret)
knn_fit <- knn3(chocolateIn ~ sugarpercent + pricepercent + winpercent, data=candy_rankings)
prob_knn <- predict(knn_fit, newdata=candy_rankings)
prob_knn
class_diag(prob_knn[,2], candy_rankings$chocolateIn, positive=1)
table(truth=factor(candy_rankings$chocolateIn==1, levels=c("TRUE", "FALSE")), predictions=factor(prob_knn[,2] >.5, levels=c("TRUE", "FALSE")))

```

```{R}
# cross-validation of np classifier

k=10
data<-candy_rankings[sample(nrow(candy_rankings)),]
folds<-cut(seq(1:nrow(candy_rankings)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$chocolateIn 
  fit <- glm(chocolateIn ~ sugarpercent + pricepercent + winpercent, data=train, family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  diags<-rbind(diags,class_diag(probs,truth,positive=1))
}
summarize_all(diags,mean)


```

Here we used k-nearest-neighbors on the same dataset to predict outcomes based on chocolate and the three numeric variables. The initial training model produced a rather good in-sample performance with an AUC of 0.9223. When it comes to predicting new observations with cross validation, the AUC was 0.89143, which did drop a bit and could be a sign of overfitting, but the drop was small so it is difficult to tell. The non-parametric model performed similarly to the linear model completed initially, however, the AUC after cross validation in the linear model proved to better fit the data. 


### Regression/Numeric Prediction

```{R}
fit <- lm(winpercent~ chocolate+fruity, data=candy_rankings)
yhat <- predict(fit)
mean((candy_rankings$winpercent-yhat)^2)
``` 

```{R}
# cross-validation of regression model here

k=5 
data<-candy_rankings[sample(nrow(candy_rankings)),]
folds<-cut(seq(1:nrow(candy_rankings)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  fit<-lm(winpercent~ chocolate+fruity,data=train)
  yhat<-predict(fit,newdata=test)
  diags<-mean((test$winpercent-yhat)^2) 
}
mean(diags)

```

For this portion of the project, I predicted winpercent from chocolate and fruity. This produced a MSE of 123.3221, which is rather large and indicates that there is likely a high amount of error in this model. After performing k-fold CV, the average MSE was 99.43919, which is still very large and confirms that the error in this model is high. The drop in MSE in cross validation is a good thing, but the fact that this value is so high means that there is likely overfitting. 

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required=F)
candyp<-candy_rankings %>% select(competitorname, winpercent) %>% group_by(competitorname) %>% arrange(desc(winpercent))
```

```{python}
# python code
candyp=r.candyp
list1 = candyp['competitorname'].tolist()
list1
list2 = candyp['winpercent'].tolist()
list2
list3= [list1,list2]
list3
```


```{python}
max = list3[1][0] 
max
min = list3[1][84]
min
```

```{r}
py$max
candyp %>% filter(winpercent == 84.18029) %>% distinct(competitorname)
py$min
candyp %>% filter(winpercent == 22.445341) %>% distinct(competitorname)
```
In this section, I first pulled out winpercent and competitorname to focus only on two variables from our dataset. I then transferred this data to python using r.candyp and transformed this data into lists so that I could then index the max and min based on their position. Then, in R, I called the max and min found in python by using py$ and then found the distinct competitor names that corresponded to these values. 

### Concluding Remarks

In conclusion, this dataset was an interesting one to examine and provided some sweet results! It appears that chocolate candies were popular in this study and it looks like chocolate may have been linked to sugar, price, and win percentage. More analysis should be done to determine whether or not the other candy features were determinants in their sugar, price, and win percentages. 




